{"cells":[{"cell_type":"markdown","metadata":{"id":"ai15eKUlzrWt"},"source":["## Library Imports"],"id":"ai15eKUlzrWt"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22819,"status":"ok","timestamp":1670195760394,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"9orjWKs54csS","outputId":"9c2f855d-7872-47e0-f20e-69da8cea3b75"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Just need once\n","from google.colab import drive\n","drive.mount('/content/drive')"],"id":"9orjWKs54csS"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1670195774410,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"pyY49p3O4kI6","outputId":"0693fdb5-f472-492c-ebe7-2c5799a9803e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1FOk_Lk6NLWAynAk-_uIaI0eDfwMGhJlM/Project05\n"]}],"source":[" %cd drive/MyDrive/Project05\n"],"id":"pyY49p3O4kI6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670195776602,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"PlXN2-fj4lI6","outputId":"4bed2f1f-fa7e-4221-9c83-caf4e5b195ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1FOk_Lk6NLWAynAk-_uIaI0eDfwMGhJlM/Project05\n"]}],"source":["# double check you are in the Project05 folder\n","! pwd"],"id":"PlXN2-fj4lI6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3676,"status":"ok","timestamp":1670195781848,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"-n2237bXzrXE","outputId":"f154a155-d9c6-4fe1-dc53-5cc18e975063"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.12.1+cu113\n","Imported\n"]}],"source":["import copy\n","import importlib\n","import json\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","from PIL import Image\n","import random\n","import time\n","import torch\n","from torchvision import transforms\n","from torchvision import datasets\n","from torch.utils.data import Subset, ConcatDataset\n","from torch.utils.tensorboard import SummaryWriter\n","print(torch.__version__)\n","\n","print(\"Imported\")"],"id":"-n2237bXzrXE"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46596,"status":"ok","timestamp":1670195890252,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"u-40osPa9wAe","outputId":"a6170169-e9f0-4d45-8a4e-ec20b8c89195"},"outputs":[{"name":"stdout","output_type":"stream","text":[" asl_alphabet_test\n"," asl_alphabet_train\n"," asl-alphabet.zip\n"," assets\n"," checkpoint\n"," checkpoint-0.pth\n"," checkpoint.pth\n","'Copy of Olivia_Sign_Alphabet_Classification_ASL-.ipynb'\n"," data\n"," environment.yml\n","'EXP1_split new.ipynb'\n","'EXP1_Split NEW.ipynb'\n"," Experiment_2.ipynb\n"," Experiment_3.ipynb\n"," fig.png\n"," Gianna_Sign_Alphabet_Classification_ASL-.ipynb\n"," image-transformations.yml\n"," jobs\n"," landscape_data\n"," logs\n"," MB_dataset\n","\"Megan's old Copy of Sign_Alphabet_Classification_ASL.ipynb\"\n"," model.py\n"," new_datasets\n"," nikipassphrase\n"," nikipassphrase.pub\n"," Niki_Sign_Alphabet_Classification_ASL-.ipynb\n","'Oct Megan_Sign_Alphabet_Classification_ASL-.ipynb'\n"," Olivia_Sign_Alphabet_Classification_ASL-.ipynb\n"," Preet_Sign_Alphabet_Classification_ASL-.ipynb\n"," process_asl_dataset.py\n"," quantization_from_tf_sign_lang.py\n"," README.md\n"," Rucha_Sign_Alphabet_Classification_ASL.ipynb\n"," Rucha_Small_SAC.ipynb\n"," Sign_Alphabet_Classification_ASL-.ipynb\n"," sign-alphabet-confusion-matrix.png\n"," sign_lang_classification_webcam.py\n","'Sign Language AI Project 2022_Maker Day #3 [Boston].pptx'\n"," small_data\n"," splitted_data\n","'Stephanie Sign_Alphabet_Classification_ASL-.ipynb'\n"," tiny-cuda-nn\n","'tiny-cuda-nn (1)'\n"," tinyengine\n"," tinynas\n"," torch_onnx_tflite.py\n"," utils\n","/content/drive/.shortcut-targets-by-id/1FOk_Lk6NLWAynAk-_uIaI0eDfwMGhJlM/Project05/tiny-cuda-nn\n","-- Targeting GPU architectures: 75\n","-- Module support is disabled.\n","-- Version: 9.0.0\n","-- Build type: Release\n","-- CXX_STANDARD: 14\n","-- Required features: cxx_variadic_templates\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/drive/.shortcut-targets-by-id/1FOk_Lk6NLWAynAk-_uIaI0eDfwMGhJlM/Project05/tiny-cuda-nn/build\n","\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target fmt\u001b[0m\n","[  4%] Built target fmt\n","\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target tiny-cuda-nn\u001b[0m\n","[ 61%] Built target tiny-cuda-nn\n","\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target bench_image_ours\u001b[0m\n","[ 76%] Built target bench_image_ours\n","\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target mlp_learning_an_image\u001b[0m\n","[ 90%] Built target mlp_learning_an_image\n","/content/drive/.shortcut-targets-by-id/1FOk_Lk6NLWAynAk-_uIaI0eDfwMGhJlM/Project05\n"]}],"source":["!ls\n","%cd tiny-cuda-nn/\n","!cmake . -B build\n","!cmake --build build --config RelWithDebInfo -j\n","%cd .."],"id":"u-40osPa9wAe"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9543,"status":"ok","timestamp":1670195906828,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"hT8_NnkrzrXW","outputId":"8a182a8f-4c81-48bb-a9eb-c26609121e8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Imported\n"]}],"source":["import tinynas.nn.networks.mobilenet_v2\n","importlib.reload(tinynas.nn.networks.mobilenet_v2)\n","from tinynas.nn.networks.mobilenet_v2 import MobileNetV2, ProxylessNASNets\n","from tinynas.nn.modules.layers import LinearLayer\n","\n","print(\"Imported\")\n"],"id":"hT8_NnkrzrXW"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3452,"status":"ok","timestamp":1670195920135,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"Uj5A7jXA0uof","outputId":"717f1363-af9e-4c3b-e623-c49d47e3da16"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.9.24)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"]}],"source":["! pip install kaggle"],"id":"Uj5A7jXA0uof"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Od9lQ9iK58KR"},"outputs":[],"source":["# run this once, obtain your own kaggle username and API key\n","import json\n","\n","!mkdir ~/.kaggle\n","!touch ~/.kaggle/kaggle.json\n","\n","api_token = {\"username\":\"giannaeverette\",\n","             \"key\":\"77e7a99b485601b2fe03cdfb267b84dd\"}\n","with open('/root/.kaggle/kaggle.json', 'w') as file:\n","    json.dump(api_token, file)"],"id":"Od9lQ9iK58KR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3zIu5fd4-ya"},"outputs":[],"source":["# ! kaggle datasets download grassknoted/asl-alphabet --unzip"],"id":"Y3zIu5fd4-ya"},{"cell_type":"markdown","metadata":{"id":"M5Y76KpdzrXX"},"source":["### Load and Build network via config"],"id":"M5Y76KpdzrXX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9IdfwttzrXX"},"outputs":[],"source":["#loading the model and initializing it with saved weights\n","file = open('assets/configs/mcunet-5fps_imagenet.json')\n","config = json.load(file)\n","model = ProxylessNASNets.build_from_config(config)\n","#checkpoint = torch.load('assets/pt_ckpt/mcunet-5fps_imagenet.pth')\n","#model.load_state_dict(checkpoint['state_dict'])"],"id":"D9IdfwttzrXX"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":239,"status":"ok","timestamp":1670195925086,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"uXb5iS9EzrXZ","outputId":"a8bfb427-44ac-4516-c7e8-495de4ce08cf"},"outputs":[{"data":{"text/plain":["{'name': 'LinearLayer',\n"," 'in_features': 160,\n"," 'out_features': 1000,\n"," 'bias': True,\n"," 'use_bn': False,\n"," 'act_func': None,\n"," 'dropout_rate': 0,\n"," 'ops_order': 'weight_bn_act'}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model.classifier.config"],"id":"uXb5iS9EzrXZ"},{"cell_type":"markdown","metadata":{"id":"EKKBbQBZzrXc"},"source":["### Preparing network for finetuning"],"id":"EKKBbQBZzrXc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Wt9kLvbzrXe"},"outputs":[],"source":["def prepare_net_for_finetune(net, num_classes_new_dataset):\n","    #freeze_layers_for_finetune(net.__class__.__name__, net)\n","    existing_classifier_config = net.classifier.config\n","    del(existing_classifier_config['name'])\n","    existing_classifier_config['out_features'] = num_classes_new_dataset\n","    existing_classifier_config['dropout_rate'] = 0.5\n","    net.classifier = LinearLayer.build_from_config(existing_classifier_config)\n","    return net\n","\n","def freeze_layers_for_finetune(name, net):\n","    if isinstance(net, torch.nn.Module):\n","        if net._modules:\n","            for layer_name, layer in net._modules.items():\n","                aggregate_name = \".\".join([name, layer_name])\n","                freeze_layers_for_finetune(aggregate_name, layer)\n","        else:\n","            if \"classifier\" not in name:\t\t# Freeze all layers except last classifier\n","                for p in net.parameters():\n","                    p.requires_grad = False\n","                    \n","net = prepare_net_for_finetune(model, 29)    ## Change Classification layer to have 29 outputs with dropout rate of 0.5, lock all other layer weights"],"id":"4Wt9kLvbzrXe"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":213,"status":"ok","timestamp":1670195938432,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"JsBWDKVnzrXf","outputId":"d034870e-8218-4097-a80d-78d5a9b6d6f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.nn.modules.dropout.Dropout'>\n","<class 'torch.nn.modules.linear.Linear'>\n"]}],"source":["for m in net.classifier._modules.values():\n","    print(type(m))"],"id":"JsBWDKVnzrXf"},{"cell_type":"markdown","metadata":{"id":"T0ut3Q3UzrXg"},"source":["### Load dataset"],"id":"T0ut3Q3UzrXg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRnOydRwzrXh"},"outputs":[],"source":["class ImageFolderWithPaths(datasets.ImageFolder):\n","    \"\"\"Custom dataset that includes image file paths. Extends\n","    torchvision.datasets.ImageFolder\n","    \"\"\"\n","\n","    # override the __getitem__ method. this is the method that dataloader calls\n","    def __getitem__(self, index):\n","        # this is what ImageFolder normally returns \n","        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n","        # the image file path\n","        path = self.imgs[index][0]\n","        # make a new tuple that includes original and the path\n","        tuple_with_path = (original_tuple + (path,))\n","        return tuple_with_path"],"id":"yRnOydRwzrXh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJ-ymccLgVJ6"},"outputs":[],"source":["from torch.utils.data import Dataset, TensorDataset,DataLoader, ConcatDataset"],"id":"CJ-ymccLgVJ6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"stVqaJh-zrXj"},"outputs":[],"source":["#load in data\n","def return_data_loader(data_dirs, batch_size, phase, resolution):\n","    # data_dirs = ['dir1', 'dir2', ...]\n","    data_transforms = {\n","        'train': transforms.Compose([\n","            transforms.Resize((resolution, resolution)),\n","            #transforms.RandomRotation(degrees=(0, 15)),\n","            transforms.RandomHorizontalFlip(),\n","            #transforms.RandomAdjustSharpness(sharpness_factor=2),\n","            #transforms.RandomAutocontrast(0.7),\n","            transforms.ColorJitter(brightness=0.3, saturation=0.3, hue=0.1),\n","            transforms.ToTensor(), \n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ]),\n","        'val': transforms.Compose([\n","            transforms.Resize((resolution, resolution)),\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ]),\n","    \n","        'test': transforms.Compose([\n","            transforms.Resize((resolution,resolution)),\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ])\n","    }\n","    # beforehand: split new data into train/ val/ test/ folders once, then use every time you train\n","\n","    final_data = ImageFolderWithPaths(os.path.join(data_dirs[0], phase), transform = data_transforms[phase])\n","\n","    for i in range(1, len(data_dirs)):\n","      additional_data = ImageFolderWithPaths(os.path.join(data_dirs[i], phase), transform = data_transforms[phase])\n","\n","      final_data = ConcatDataset([final_data, additional_data])\n","\n","  \n","    # additional_data = ImageFolderWithPaths(os.path.join('/content/drive/MyDrive/Project05/new_datasets/ASL_Dataset', phase), transform = data_transforms[phase])\n","\n","\n","    if phase == 'train':\n","        data = ImageFolderWithPaths(os.path.join(data_dirs[0], 'train'), transform = data_transforms[phase])\n","        train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True,\n","                                                num_workers=4)\n","        \n","        return train_loader\n","    \n","    elif phase == \"val\":\n","        data = ImageFolderWithPaths(os.path.join(data_dirs[0], 'train'), transform = data_transforms[phase])\n","        val_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=False, drop_last=False,\n","                                        num_workers=4)\n","        return val_loader\n","    \n","    else: \n","        test_loader = torch.utils.data.DataLoader(final_data, batch_size=batch_size, shuffle=False, drop_last=False,\n","                                                    num_workers=4)\n","        return test_loader"],"id":"stVqaJh-zrXj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIngQJKML22u"},"outputs":[],"source":["# # testing new data \n","# import tensorflow as tf \n","# import os\n","# import cv2\n","# import imghdr\n","\n","\n","# data_dir ='/content/drive/MyDrive/Project05/new_datasets/ASL_Dataset/train'\n","# image_exts=['jpeg','jpg','bmp','png']\n","# image_exts[2]\n"],"id":"fIngQJKML22u"},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYJn3KzexxF2"},"outputs":[],"source":["# for image_class in os.listdir(data_dir): \n","#     for image in os.listdir(os.path.join(data_dir, image_class)):\n","#         image_path = os.path.join(data_dir, image_class, image)\n","#         try: \n","#             img = cv2.imread(image_path)\n","#             tip = imghdr.what(image_path)\n","#             if tip not in image_exts: \n","#                 print('Image not in ext list {}'.format(image_path))\n","#                 os.remove(image_path)\n","#         except Exception as e: \n","#             print('Issue with image {}'.format(image_path))"],"id":"DYJn3KzexxF2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKBDgyxxPpkh"},"outputs":[],"source":["# # tried to upload the new data and show pics\n","# import numpy as np\n","# from matplotlib import pyplot as plt\n","# import tensorflow as tf\n","# data = tf.keras.utils.image_dataset_from_directory('new_datasets')"],"id":"uKBDgyxxPpkh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDXweyeDP3Dz"},"outputs":[],"source":["# data_iterator = data.as_numpy_iterator()"],"id":"EDXweyeDP3Dz"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFYQcaAaP3LM"},"outputs":[],"source":["# batch = data_iterator.next()"],"id":"kFYQcaAaP3LM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"84-7i5YeP7pC"},"outputs":[],"source":["# fig, ax = plt.subplots(ncols=4, figsize=(15,20))\n","# for idx, img in enumerate(batch[0][:4]):\n","#     ax[idx].imshow(img.astype(int))\n","#     ax[idx].title.set_text(batch[1][idx])"],"id":"84-7i5YeP7pC"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FHZGoXvSR7w"},"outputs":[],"source":["# Scale the data\n","# data = data.map(lambda x,y: (x/255, y))\n","# data.as_numpy_iterator().next()"],"id":"6FHZGoXvSR7w"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1670197123536,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"XMR3DK41Sabh","outputId":"c5e5041d-d67d-4b14-be1e-2c2b3a8fe0d1"},"outputs":[{"data":{"text/plain":["1914"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# train_size = int(len(data)*.7)\n","# val_size = int(len(data)*.2)\n","# test_size = int(len(data)*.1)\n","# train_size"],"id":"XMR3DK41Sabh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"umI-fgWfSgZB"},"outputs":[],"source":["# train = data.take(train_size)\n","# val = data.skip(train_size).take(val_size)\n","# test = data.skip(train_size+val_size).take(test_size)"],"id":"umI-fgWfSgZB"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DwvPEeOSmbC"},"outputs":[],"source":["#import cv2\n","#img = cv2.imread('a.jpg')\n","#plt.imshow(img)\n","#plt.show()"],"id":"1DwvPEeOSmbC"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jHP6OORzrXm"},"outputs":[],"source":["#loading data\n","all_data_dirs = ['./splitted_data', 'new_datasets/ASL_Dataset'] #can change the splitted data\n","target_data_dir = ['splitted_data']   ## May need to change it to the path where you have ASL stored after running process_asl_dataset.py\n","train_loader = return_data_loader(target_data_dir, 20, phase = 'train', resolution=96)\n","# val_loader = return_data_loader(target_data_dir, 256, phase = 'val', resolution=96)\n","\n","# print(\"Class Label to Network Output Index {}\".format(train_loader.dataset.class_to_idx))"],"id":"7jHP6OORzrXm"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1670197592189,"user":{"displayName":"Gianna Everette","userId":"03470458488941016326"},"user_tz":300},"id":"Csy6jJALOibR","outputId":"a0fa9583-3d6a-462f-f78f-181bb95ac096"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1FOk_Lk6NLWAynAk-_uIaI0eDfwMGhJlM/Project05\n"]}],"source":["!pwd"],"id":"Csy6jJALOibR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NG57XUtnHDh6"},"outputs":[],"source":["loader_iter = iter(train_loader)"],"id":"NG57XUtnHDh6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YhSSYz9s-yWF"},"outputs":[],"source":["batch = next(loader_iter)[0]"],"id":"YhSSYz9s-yWF"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mD_unjA_GU7U"},"outputs":[],"source":["# f, ax = plt.subplots(1, 4, figsize=(20, 5))\n","\n","# for i, data in enumerate(batch):\n","#   ax[i].imshow(data.permute(0, 2, 0))"],"id":"mD_unjA_GU7U"},{"cell_type":"markdown","metadata":{"id":"u6qM5k6hzrXn"},"source":["### Train Model"],"id":"u6qM5k6hzrXn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iS0WTxQvXQ8_"},"outputs":[],"source":["from tqdm import tqdm"],"id":"iS0WTxQvXQ8_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYA8uEcrzrYA"},"outputs":[],"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs, writer, weights_dir):\n","    since = time.time()\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        start_epoch = time.time()\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            with tqdm(total=len(dataloaders[phase]), position=0, leave=True) as pbar:\n","              for data in tqdm(dataloaders[phase], position=0, leave=True):\n","                  inputs, labels, paths = data\n","                  inputs = inputs.to(device)\n","                  labels = labels.to(device)\n","\n","                  # zero the parameter gradients\n","                  optimizer.zero_grad()\n","\n","                  # forward + track history if only in train\n","                  with torch.set_grad_enabled(phase == 'train'):\n","                      # Get model outputs and calculate loss\n","                      outputs = model(inputs)\n","                      loss = criterion(outputs, labels)\n","\n","                      _, preds = torch.max(outputs, 1)\n","\n","                      # backward + optimize only if in training phase\n","                      if phase == 'train':\n","                          loss.backward()\n","                          optimizer.step()\n","\n","                  # statistics\n","                  running_loss += loss.item() * inputs.size(0)\n","                  running_corrects += torch.sum(preds == labels.data)\n","                  pbar.update()\n","\n","            #printing and writing to tensorboard loss and accuracy\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","            writer.add_scalar(phase + ' loss', epoch_loss, epoch)\n","            writer.add_scalar(phase + ' accuracy', epoch_acc, epoch)\n","            \n","            #printing time per epoch\n","            time_elapsed_epoch = time.time() - start_epoch\n","            print('Epoch complete in {:.0f}m {:.0f}s'.format(time_elapsed_epoch // 60, time_elapsed_epoch % 60))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict()) \n","                #save model weights to best weights_dir\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': criterion,\n","                    }, weights_dir + \"best_weights/model_\" + str(epoch + 1) + \".pth\")\n","            if phase == 'val':\n","                #save model weights after every epoch\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': criterion,\n","                    }, weights_dir + \"model_\" + str(epoch + 1) + \".pth\")\n","                \n","\n","        print()\n","        \n","    writer.flush()\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"id":"vYA8uEcrzrYA"},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-HslJm7zrYB"},"outputs":[],"source":["#creating optimizer\n","optimizer_ft = torch.optim.Adam(net.parameters(), lr = 0.001, weight_decay=0.001) #weight_decay = 0.00004)\n","\n","#setting loss function\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","#detecting if there's a gpu available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","net = net.to(device)"],"id":"U-HslJm7zrYB"},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXzsuDgWzrYC"},"outputs":[],"source":["#setting up tensorboard\n","# ! mkdir logs/\n","writer = SummaryWriter('logs/gianna_test_run')"],"id":"XXzsuDgWzrYC"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ryIJOCGrzrYD","outputId":"b01b294c-a3e9-4ab9-d022-b67ef5dbb7ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/0\n","----------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3062/3062 [1:53:18<00:00,  2.22s/it]\n","100%|██████████| 3062/3062 [1:53:18<00:00,  2.22s/it]"]},{"name":"stdout","output_type":"stream","text":["train Loss: 1.6403 Acc: 0.4764\n","Epoch complete in 113m 19s\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"KeyError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-d5aa6aa860e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/best_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-56-85a6343a63b3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, writer, weights_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'val'"]}],"source":["#training and evaluating on validation dataset\n","#dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n","dataloaders_dict = {\"train\": train_loader}\n","weights_dir = \"logs/gianna_test_run_new/\"\n","if not os.path.exists(weights_dir):\n","    os.mkdir(weights_dir)\n","    os.mkdir(weights_dir + \"/best_weights\")\n","model_ft = train_model(net, dataloaders_dict, criterion, optimizer_ft, 1, writer, weights_dir)\n"],"id":"ryIJOCGrzrYD"},{"cell_type":"markdown","metadata":{"id":"vTgJdpkGzrYE"},"source":["## Testing the fine tuned model"],"id":"vTgJdpkGzrYE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZP-LnLKJPYs"},"outputs":[],"source":[],"id":"oZP-LnLKJPYs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"-53N3I7azrYF"},"outputs":[],"source":["def test_model(model, test_loader):\n","    model.eval()\n","    \n","    correct = 0\n","    total = 0\n","    all_preds = torch.tensor([])\n","    all_preds = all_preds.to(device)\n","    all_labels = torch.tensor([])\n","    all_labels = all_labels.to(device)\n","    all_paths = []\n","    \n","    with torch.no_grad():\n","        for i, data in enumerate(test_loader):\n","            inputs, labels, paths = data\n","            \n","            \n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # calculate outputs by running images through the network\n","            outputs = model(inputs)\n","\n","            # the class with the highest energy is what we choose as prediction\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            all_preds = torch.cat(\n","                (all_preds, predicted)\n","                ,dim=0\n","            )\n","\n","            all_labels = torch.cat(\n","                (all_labels, labels)\n","                ,dim=0\n","            )\n","            \n","            all_paths = all_paths + list(paths)\n","            \n","    acc = 100 * correct / total\n","    print('Accuracy of the network on the test images: %d %%' % (acc))\n","    return acc, all_preds, all_labels, all_paths"],"id":"-53N3I7azrYF"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEMvAmoUzrYF"},"outputs":[],"source":["#loading the model for testing and initializing it with the best weights\n","file = open('assets/configs/mcunet-5fps_imagenet.json')\n","config = json.load(file)\n","model = ProxylessNASNets.build_from_config(config)\n","model_ft = prepare_net_for_finetune(model, 29)\n","\n","## Go to the weights folder and check the best weights checkpoint (add it here)\n","checkpoint = torch.load('logs/preet_test_run/best_weights/model_1.pth')\n","model_ft.load_state_dict(checkpoint['model_state_dict'])\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_ft = model_ft.to(device)"],"id":"eEMvAmoUzrYF"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1141,"status":"ok","timestamp":1665254735320,"user":{"displayName":"preet Kaur","userId":"14195017154629494478"},"user_tz":240},"id":"Y5G_JncfTndw","outputId":"3b60a68d-f185-45b2-e278-68d41cb1cf77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"Y5G_JncfTndw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMHCTvJozrYG"},"outputs":[],"source":["## Test loader and testing\n","print(all_data_dirs)\n","test_loader = return_data_loader(all_data_dirs, 20, phase = 'test', resolution=96)\n","acc, all_preds, all_labels, all_paths = test_model(model_ft, test_loader)"],"id":"EMHCTvJozrYG"},{"cell_type":"markdown","metadata":{"id":"NSIIT4w8zrYI"},"source":["### Confusion Matrix"],"id":"NSIIT4w8zrYI"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"elapsed":317,"status":"error","timestamp":1665244616539,"user":{"displayName":"preet Kaur","userId":"14195017154629494478"},"user_tz":240},"id":"cagV0ZouzrYJ","outputId":"c3d5c352-fb3d-4b62-b56e-b6615a850cb1"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-55-ecf2489554d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'all_preds' is not defined"]}],"source":["#confusion matrix for har_up and montreal dataset\n","from sklearn.metrics import confusion_matrix\n","import seaborn\n","\n","y_preds = all_preds.cpu().data.numpy()\n","y_labels = all_labels.cpu().data.numpy()\n","\n","#print(len(class_names))\n","#labels = ['empty', 'full']\n","labels = train_loader.dataset.class_to_idx.keys()\n","data = confusion_matrix(y_labels, y_preds)\n","\n","seaborn.set(color_codes=True)\n","plt.figure(1, figsize=(54, 36))\n","\n","plt.title(\"Confusion Matrix\")\n","\n","seaborn.set(font_scale=1.4)\n","ax = seaborn.heatmap(data, annot=True, cmap=\"Blues\", fmt = 'g', cbar_kws={'label': 'Scale'})\n","\n","ax.set_xticklabels(labels)\n","ax.set_yticklabels(labels)\n","\n","ax.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n","\n","#plt.show()\n","plt.savefig('./sign-alphabet-confusion-matrix.png')"],"id":"cagV0ZouzrYJ"},{"cell_type":"markdown","metadata":{"id":"EsL595wezrYQ"},"source":["### Classification Report"],"id":"EsL595wezrYQ"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":1721,"status":"error","timestamp":1669755787244,"user":{"displayName":"preet Kaur","userId":"14195017154629494478"},"user_tz":300},"id":"Pm574dtgzrYQ","outputId":"3e37241c-9980-41c4-e662-7932eba421a4"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ba98a513c9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"]}],"source":["\n","from sklearn.metrics import classification_report\n","target_names = train_loader.dataset.class_to_idx.keys()\n","print(classification_report(y_labels, y_preds, target_names = target_names))"],"id":"Pm574dtgzrYQ"},{"cell_type":"markdown","metadata":{"id":"ba9zOrkSzrYR"},"source":["## Testing on actual test images that came with the dataset"],"id":"ba9zOrkSzrYR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4oo6rSSzrYR"},"outputs":[],"source":["def load_images_and_return_batch_tensor(sample_img_dir):\n","    images_in_dir = os.listdir(sample_img_dir)\n","    img_paths = map(lambda x : os.path.join(sample_img_dir, x), images_in_dir)\n","\n","    list_of_imgs = []\n","\n","    for img_pth in img_paths:\n","        image = Image.open(img_pth)\n","        #x = TF.to_tensor(image)\n","        #x.unsqueeze_(0)\n","        list_of_imgs.append((img_pth, image))\n","\n","    return list_of_imgs\n","\n","resolution = 96\n","\n","data_transforms = transforms.Compose([\n","    transforms.Resize((resolution,resolution)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","img_name_to_pred_dict = {}\n","\n","sample_img_dir = \"../asl-dataset-sign/test\"        ## Change it to path where you have ASL test images stored (before running process_asl_dataset.py)\n","list_of_imgs = load_images_and_return_batch_tensor(sample_img_dir)\n","\n","inv_map = {v: k for k, v in train_loader.dataset.class_to_idx.items()}\n","\n","for img_name, img in list_of_imgs:\n","    img = data_transforms(img)\n","    img = img.unsqueeze_(0)\n","    img = img.to(device)\n","    output = model_ft(img)\n","\n","    _, predicted = torch.max(output.data, 1)\n","\n","    img_name_to_pred_dict[img_name] = str(predicted.cpu().numpy()[0])\n","    print(\"Image name : {}, Prediction : {}\".format(img_name, inv_map[predicted.cpu().numpy()[0]]))\n","    #print(\"Image prediction : {}\".format(inv_map[predicted.cpu().numpy()[0]]))"],"id":"C4oo6rSSzrYR"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ZmOfZU0UqkUClrdL5Y1H2sqUBBu5a-iV","timestamp":1670191551565},{"file_id":"1SlkpUYyyUakcq-A9YzSczn908hQGilso","timestamp":1670190197462},{"file_id":"1Aaj5g3lpb6aCHtDcNeC0JU4Obj89qK2M","timestamp":1670182499231}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":5}